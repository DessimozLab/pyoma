#!/usr/bin/env python

import tables
import numpy
import pyoma.browser.convert
from pyoma.browser import suffixsearch
import logging

logger = logging.getLogger("add-xrefs")


def add_xrefs_and_rebuild_indexes(filename, tsv_paths):
    exporter = pyoma.browser.convert.DarwinExporter(filename, mode="append")
    nodes_to_remove = [
        "/_si_XRef",
    ]
    for node in nodes_to_remove:
        logger.info('removing node "{}"'.format(node))
        try:
            exporter.h5.remove_node(node, recursive=True)
        except tables.NoSuchNodeError:
            pass

    xref_tab = exporter.h5.get_node("XRef")
    dtype = xref_tab.dtype
    for fname in tsv_paths:
        rows = numpy.fromfile(fname, dtype=dtype, sep="\t")
        xref_tab.append(rows)
    xref_tab.flush()
    exporter.close()

    exporter = pyoma.browser.convert.DarwinExporter(filename, mode="append")
    logger.info("creating index for xrefs (EntryNr and XRefId)")
    xrefTab = exporter.h5.get_node("/XRef")
    pyoma.browser.convert.create_index_for_columns(xrefTab, "EntryNr", "XRefId")
    logger.info("creating suffix index for XRefId")
    suffixsearch.create_suffix_index(xrefTab, "XRefId")
    exporter.close()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Add additional xrefs from tsv files and update xref indices"
    )
    parser.add_argument("--db", required=True, help="Path to the hdf5 database")
    parser.add_argument(
        "--tsv", nargs="*", help="path to additional tsv files to be included"
    )
    conf = parser.parse_args()
    logging.basicConfig(level=logging.INFO)

    add_xrefs_and_rebuild_indexes(conf.hdf5, conf.tsv)
